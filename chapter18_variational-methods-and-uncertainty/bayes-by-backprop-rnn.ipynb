{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop with ``gluon`` (RNNs for sequence prediction)\n",
    "\n",
    "In this chapter, we apply [Bayes by Backprop](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop-gluon.ipynb) ``(BBB)`` to a more challenging modeling problem, learning [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb) for sequence prediction.\n",
    "\n",
    "As we've seen, Bayes-by-backprop lets us fit expressive models efficiently and lets us represent uncertainty about  model parameters. Representing uncertainty not only helps us avoid overfitting, it is an important input to sound decision making.\n",
    "\n",
    "Thankfully, ``BBB`` for RNNs is not much more difficult than the feed-forward case. It really just requires swapping a recurrent neural network for the feed-forward one and a changing the log-likelihood to something appropriate for sequence modeling.\n",
    "\n",
    "In what follows, we reimplement the sequence model from [''Bayesian Recurrent Neural Networks'', by Fortunato et al.](https://arxiv.org/pdf/1704.02798.pdf) and rerun the authors' experiments on the Penn Tree-Bank dataset, which you may recall we used in the Straight-Dope chapter, [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb).\n",
    "\n",
    "If you have not looked at the chapters [Bayes by Backprop](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop-gluon.ipynb) or [Recurrent Neural Networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb), it is worth doing so since we reuse a lot that code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Initialize Configuration and Hyperparameters\n",
    "\n",
    "Before starting, we make some necessary package imports, perform basic configuration and initialize model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(0)\n",
    "args_data = '../data/nlp/ptb.'\n",
    "args_model = 'lstm'\n",
    "args_emsize = 100\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_lr = 10.0\n",
    "args_clip = 0.2\n",
    "args_epochs = 1\n",
    "args_batch_size = 32\n",
    "args_bptt = 5\n",
    "args_dropout = 0.2\n",
    "args_tied = True\n",
    "args_cuda = 'store_true'\n",
    "args_log_interval = 500\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classes for Loading the Language Data\n",
    "\n",
    "First, we need to load and access the Penn Tree-Bank data. Here we use the classes we defined in the Straight-Dope chapter on [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')\n",
    "\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Penn Tree-Bank Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(args_data)\n",
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)\n",
    "val_data = batchify(corpus.valid, args_batch_size).as_in_context(context)\n",
    "test_data = batchify(corpus.test, args_batch_size).as_in_context(context)\n",
    "num_batches = int(np.ceil( (train_data.shape[0] - 1)/args_bptt) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Recurrent Neural Network Model\n",
    "\n",
    "Next we resurrect the recurrent neural network model we saw in [the chapter on recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb).\n",
    "\n",
    "The class here is defined identically, except we've added a convenience method, ``set_params_to``, which is used by Bayes-by-backprop to set the parameters of the RNN to ones sampled from the variational posterior.\n",
    "\n",
    "As in [the chapter on recurrent neural nets](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb), we define the auxiliary function, ``detach``, which we use to detach the hidden state from the computation graph. By detaching the hidden state after each minibatch, we relieve MXNet of trying to back-propagate the gradient across minibatches, and thus indefinitely far back in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu',\n",
    "                                   dropout=dropout, input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "\n",
    "    def set_params_to(self, new_values):\n",
    "        for model_param, new_value in zip(self.collect_params().values(), new_values):\n",
    "            model_param_ctx = model_param.list_ctx()[0]\n",
    "            model_param._data[ model_param_ctx ] = new_value\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Initialize Our Favorite Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    model.collect_params(), 'sgd',\n",
    "    {'learning_rate': args_lr, 'momentum': 0, 'wd': 0}\n",
    ")\n",
    "\n",
    "smce_loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN Training and Evaluation Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_baseline():\n",
    "    global args_lr\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = smce_loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size * args_bptt)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = evaluate(val_data, model)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(test_data, model)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)\n",
    "    return\n",
    "\n",
    "\n",
    "def evaluate(data_source, model):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = smce_loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the LSTM Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 6.73, perplexity 834.37\n",
      "[Epoch 1 Batch 1000] loss 6.14, perplexity 462.27\n",
      "[Epoch 1 Batch 1500] loss 5.89, perplexity 360.56\n",
      "[Epoch 1 Batch 2000] loss 5.81, perplexity 333.35\n",
      "[Epoch 1 Batch 2500] loss 5.68, perplexity 292.76\n",
      "[Epoch 1 Batch 3000] loss 5.56, perplexity 260.22\n",
      "[Epoch 1 Batch 3500] loss 5.56, perplexity 260.70\n",
      "[Epoch 1 Batch 4000] loss 5.43, perplexity 227.89\n",
      "[Epoch 1 Batch 4500] loss 5.41, perplexity 222.73\n",
      "[Epoch 1 Batch 5000] loss 5.40, perplexity 222.27\n",
      "[Epoch 1 Batch 5500] loss 5.41, perplexity 223.43\n",
      "[Epoch 1] time cost 56.38s, validation loss 5.30, validation perplexity 199.93\n",
      "test loss 5.27, test perplexity 194.90\n",
      "Best test loss 5.27, test perplexity 194.90\n"
     ]
    }
   ],
   "source": [
    "train_rnn_baseline()\n",
    "model.load_params(args_save, context)\n",
    "test_L = evaluate(test_data, model)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bayes-by-Backprop Classes\n",
    "\n",
    "Now we begin the Bayes-by-backprop portion of the chapter in earnest. To start with, we define the familiar log-probability of the Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_prob(x, mu, sigma):\n",
    "    return - mx.nd.log(sigma) - (x - mu) ** 2 / (2 * sigma ** 2)\n",
    "\n",
    "def gaussian_prob(x, mu, sigma):\n",
    "    scaling = 1.0 / mx.nd.sqrt(2.0 * np.pi * (sigma ** 2))\n",
    "    bell = mx.nd.exp(-(x - mu)**2 / (2.0 * sigma ** 2))\n",
    "    return scaling * bell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our Prior\n",
    "\n",
    "Next, we define a Gaussian prior over the RNN model's parameters. It is initialized with a prior mean and standard deviation $\\sigma$, and has one important method, ``log_prob``, which computes the log-probability of the RNN model's paramters under the given prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior1(object):\n",
    "\n",
    "    def __init__(self, prior_mu, prior_sigma):\n",
    "        self.prior_mu = mx.nd.array([prior_mu], ctx=context)\n",
    "        self.prior_sigma = mx.nd.array([prior_sigma], ctx=context)\n",
    "        return\n",
    "\n",
    "    def log_prob(self, model_params):\n",
    "        log_probs = [\n",
    "            mx.nd.sum(log_gaussian_prob(model_param, self.prior_mu, self.prior_sigma))\n",
    "            for model_param in model_params\n",
    "        ]\n",
    "        total_log_prob = log_probs[0]\n",
    "        for log_prob in log_probs[1:]:\n",
    "            total_log_prob = total_log_prob + log_prob\n",
    "        return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior2(object):\n",
    "\n",
    "    def __init__(self, alpha, sigma1, sigma2):\n",
    "        self.alpha = mx.nd.array([alpha], ctx=context)\n",
    "        self.one_minus_alpha = mx.nd.array([1 - alpha], ctx=context)\n",
    "        self.zero = mx.nd.array([0.0], ctx=context)\n",
    "        self.sigma1 = mx.nd.array([sigma1], ctx=context)\n",
    "        self.sigma2 = mx.nd.array([sigma2], ctx=context)\n",
    "        return\n",
    "\n",
    "    def log_prob(self, model_params):\n",
    "        total_log_prob = None\n",
    "        for i, model_param in enumerate(model_params):\n",
    "            p1 = gaussian_prob(model_param, self.zero, self.sigma1)\n",
    "            p2 = gaussian_prob(model_param, self.zero, self.sigma2)\n",
    "            log_prob = mx.nd.sum(mx.nd.log(self.alpha * p1 + self.one_minus_alpha * p2))\n",
    "            if i == 0: total_log_prob = log_prob\n",
    "            else: total_log_prob = total_log_prob + log_prob\n",
    "        return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Variational Posterior\n",
    "\n",
    "Now we are getting into the guts of Bayes-by-backprop. Next we define the variational posterior class. Like the prior class, the variational posterior is able to compute the log-probability of the RNN's parameters under its variational posterior distribution.\n",
    "\n",
    "However, unlike the prior class, the variational posterior's parameters get updated during training, whereas the prior's parameters are never updated. They are, after all, \"prior\".\n",
    "\n",
    "Additionally, the variational posterior provides a method to sample the RNN's parameters from its current posterior distribution. This is done with the ``sample_model_params`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarPosterior2(object):\n",
    "\n",
    "    def __init__(self, model, var_mu_init_scale, var_sigma_init_scale):\n",
    "        self.var_mus = []\n",
    "        self.var_rhos = []\n",
    "        self.raw_var_mus = []\n",
    "        self.raw_var_rhos = []\n",
    "        var_rho_init_scale = inv_softplus(var_sigma_init_scale)\n",
    "\n",
    "        for i, model_param in enumerate(model.collect_params().values()):\n",
    "\n",
    "            var_mu = gluon.Parameter(\n",
    "                'var_mu_{}'.format(i), shape=model_param.shape,\n",
    "                init=mx.init.Normal(var_mu_init_scale))\n",
    "            var_mu.initialize(ctx=context)\n",
    "            self.var_mus.append(var_mu)\n",
    "            self.raw_var_mus.append(var_mu.data(context))\n",
    "\n",
    "            var_rho = gluon.Parameter(\n",
    "                'var_rho_{}'.format(i), shape=model_param.shape,\n",
    "                init=mx.init.Constant(var_rho_init_scale))\n",
    "            var_rho.initialize(ctx=context)\n",
    "            self.var_rhos.append(var_rho)\n",
    "            self.raw_var_rhos.append(var_rho.data(context))\n",
    "\n",
    "        self.var_params = self.var_mus + self.var_rhos\n",
    "        return\n",
    "\n",
    "    def sample_model_params(self):\n",
    "        model_params = []\n",
    "        for raw_var_mu, raw_var_rho in zip(self.raw_var_mus, self.raw_var_rhos):\n",
    "            epsilon = mx.nd.random_normal(shape=raw_var_mu.shape, loc=0., scale=1.0, ctx=context)\n",
    "            var_sigma = softplus(raw_var_rho)\n",
    "            model_param = raw_var_mu + var_sigma * epsilon\n",
    "            model_params.append(model_param)\n",
    "        return model_params\n",
    "\n",
    "    def log_prob(self, model_params):\n",
    "        log_probs = [\n",
    "            mx.nd.sum(log_gaussian_prob(model_param, raw_var_mu, softplus(raw_var_rho)))\n",
    "            for (model_param, raw_var_mu, raw_var_rho)\n",
    "            in zip(model_params, self.raw_var_mus, self.raw_var_rhos)\n",
    "        ]\n",
    "        total_log_prob = log_probs[0]\n",
    "        for log_prob in log_probs[1:]:\n",
    "            total_log_prob = total_log_prob + log_prob\n",
    "        return total_log_prob\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum([\n",
    "            2 * np.prod(param.shape)\n",
    "            for param in self.var_mus\n",
    "        ])\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return mx.nd.log(1. + mx.nd.exp(x))\n",
    "\n",
    "def inv_softplus(x):\n",
    "    if x <= 0: raise ValueError(\"x must be > 0: {}\".format(x))\n",
    "    return np.log(np.exp(x) - 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Bayes-by-Backprop Loss\n",
    "\n",
    "As discussed, the Bayes-by-backprop loss is the sum of the expected log-likelihood on the training data and the KL-divergence between the variational posterior and the prior:\n",
    "\n",
    " ``INSERT FORMULA HERE``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBB_RNN_Loss(gluon.loss.Loss):\n",
    "\n",
    "    def __init__(self, prior, var_posterior, log_likelihood, num_batches, weight=None, batch_axis=0, **kwargs):\n",
    "        super(BBB_RNN_Loss, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self.prior = prior\n",
    "        self.var_posterior = var_posterior\n",
    "        self.log_likelihood = log_likelihood\n",
    "        self.num_batches = num_batches\n",
    "        return\n",
    "    \n",
    "    def forward(self, yhat, y, sampled_params, sample_weight=None):\n",
    "        neg_log_likelihood = mx.nd.sum(self.log_likelihood(yhat, y))\n",
    "        prior_log_prob = mx.nd.sum(self.prior.log_prob(sampled_params))\n",
    "        var_post_log_prob = mx.nd.sum(self.var_posterior.log_prob(sampled_params))\n",
    "        kl_loss = var_post_log_prob - prior_log_prob\n",
    "        var_loss = neg_log_likelihood + kl_loss / self.num_batches\n",
    "        return var_loss, neg_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize BBB-relevant classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, dropout=0.0, tie_weights=args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "#prior = Prior1(0.0, 1)\n",
    "prior = Prior2(0.5, 0.001, 1)\n",
    "\n",
    "var_posterior = VarPosterior2(model, 0.05, 0.01)\n",
    "\n",
    "\n",
    "bbb_rnn_loss = BBB_RNN_Loss(prior,\n",
    "                            var_posterior,\n",
    "                            gluon.loss.SoftmaxCrossEntropyLoss(),\n",
    "                            num_batches)\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    var_posterior.var_params, 'sgd',\n",
    "    { 'learning_rate': args_lr, 'momentum': 0, 'wd': 0 }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the BBB-RNN training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_bbb():\n",
    "    global args_lr\n",
    "    global args_ess_multiplier\n",
    "    best_val = float(\"Inf\")\n",
    "\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            x, y = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "\n",
    "            with autograd.record():\n",
    "                sampled_params = var_posterior.sample_model_params()\n",
    "                model.set_params_to(sampled_params)\n",
    "                yhat, hidden = model(x, hidden)\n",
    "                var_loss, L = bbb_rnn_loss(yhat, y, sampled_params)\n",
    "                var_loss.backward()\n",
    "\n",
    "            grads = [var_mu.grad(context) for var_mu in var_posterior.var_mus]\n",
    "            effective_batch_size = (args_bptt * args_batch_size) + (var_posterior.num_params() / num_batches)\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * effective_batch_size)\n",
    "            trainer.step(args_clip * effective_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        model.set_params_to(var_posterior.raw_var_mus)\n",
    "        val_L = evaluate(val_data, model)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            model.set_params_to(var_posterior.raw_var_mus)\n",
    "            test_L = evaluate(test_data, model)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start BBB training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 6.57, perplexity 715.70\n",
      "[Epoch 1 Batch 1000] loss 5.90, perplexity 365.72\n",
      "[Epoch 1 Batch 1500] loss 5.65, perplexity 283.50\n",
      "[Epoch 1 Batch 2000] loss 5.58, perplexity 265.31\n",
      "[Epoch 1 Batch 2500] loss 5.46, perplexity 234.32\n",
      "[Epoch 1 Batch 3000] loss 5.33, perplexity 206.37\n",
      "[Epoch 1 Batch 3500] loss 5.33, perplexity 207.27\n",
      "[Epoch 1 Batch 4000] loss 5.21, perplexity 182.57\n",
      "[Epoch 1 Batch 4500] loss 5.18, perplexity 177.41\n",
      "[Epoch 1 Batch 5000] loss 5.19, perplexity 179.04\n",
      "[Epoch 1 Batch 5500] loss 5.21, perplexity 182.24\n",
      "[Epoch 1] time cost 275.05s, validation loss 5.25, validation perplexity 189.98\n",
      "test loss 5.21, test perplexity 182.65\n",
      "Best test loss 5.21, test perplexity 182.65\n"
     ]
    }
   ],
   "source": [
    "train_rnn_bbb()\n",
    "model.load_params(args_save, context)\n",
    "model.set_params_to(var_posterior.raw_var_mus)\n",
    "test_L = evaluate(test_data, model)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
