{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop with ``gluon`` (RNNs for sequence prediction)\n",
    "\n",
    "In this chapter, we apply [Bayes by Backprop](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop-gluon.ipynb) ``(BBB)`` to learning [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb) for sequence prediction.\n",
    "\n",
    "As we've seen, Bayes-by-backprop lets us fit expressive models efficiently and lets us represent uncertainty about  model parameters. Representing uncertainty helps avoid overfitting and is an important part of sound decision making.\n",
    "\n",
    "Thankfully, ``BBB`` for RNNs is not much more difficult than in the feed-forward case. It mainly requires swapping a recurrent neural network for a feed-forward one and changing the log-likelihood to something appropriate for sequence modeling.\n",
    "\n",
    "In what follows, we reimplement the sequence model from [''Bayesian Recurrent Neural Networks'', by Fortunato et al.](https://arxiv.org/pdf/1704.02798.pdf) and recreate the authors' experiments on the Penn Tree-Bank dataset, which we also used in the Straight-Dope chapter, [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb).\n",
    "\n",
    "If you have not looked at the chapters [Bayes by Backprop](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop-gluon.ipynb) or [Recurrent Neural Networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb), it is worth doing so since we reuse a lot that code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "To begin with, we need to make the following necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Configuration Variables and Hyperparameters\n",
    "\n",
    "Next we perform basic configuration and initialize model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = mx.gpu(0)\n",
    "#args_home = '/home/ubuntu'\n",
    "#args_data_root = args_home + '/mxnet-the-straight-dope/data'\n",
    "#args_data = args_data_root + '/nlp/ptb.'\n",
    "context = mx.cpu(0)\n",
    "args_data = '../data/nlp/ptb.'\n",
    "args_model = 'lstm'\n",
    "args_emsize = 100\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_lr = 1.0\n",
    "args_clip = 0.2\n",
    "args_epochs = 1\n",
    "args_batch_size = 32\n",
    "args_bptt = 5\n",
    "args_tied = True\n",
    "args_cuda = 'store_true'\n",
    "args_log_interval = 500\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classes for Loading the Language Data\n",
    "\n",
    "To load and access the Penn Tree-Bank data, we use the classes we defined in the Straight-Dope chapter on [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb). As in that notebook, the ``Dictionary`` class maps words (i.e. strings) to numeric token IDs. And the ``Corpus`` class stores the sequence of words in the training, validation and testing sets. The functions ``batchify`` and ``get_batch`` allow us to iterate over the corpus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')\n",
    "\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Recurrent Neural Network Class\n",
    "\n",
    "Next we define the recurrent neural network class which we saw in the chapter [recurrent neural networks](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/rnns-gluon.ipynb).\n",
    "\n",
    "We've added a helper method, ``set_params_to``, which is used to set the parameters of the RNN to ones sampled from a variational posterior.\n",
    "\n",
    "We also define an auxiliary function, ``detach``, which we use to detach the hidden state from the computation graph after every minibatch. By detaching the hidden state after each minibatch, we relieve MXNet of trying to propagate  the gradient indefinitely far back in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu',\n",
    "                                   dropout=dropout, input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "\n",
    "    def set_params_to(self, new_values):\n",
    "        for model_param, new_value in zip(self.collect_params().values(), new_values):\n",
    "            model_param_ctx = model_param.list_ctx()[0]\n",
    "            model_param._data[ model_param_ctx ] = new_value\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Penn Tree-Bank Corpus\n",
    "\n",
    "By now we should be familiar with the Penn Tree-Bank corpus. It's basically the MNIST of sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(args_data)\n",
    "ntokens = len(corpus.dictionary)\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)\n",
    "val_data = batchify(corpus.valid, args_batch_size).as_in_context(context)\n",
    "test_data = batchify(corpus.test, args_batch_size).as_in_context(context)\n",
    "num_batches = int(np.ceil( (train_data.shape[0] - 1)/args_bptt) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Initialize Our Favorite Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dropout = 0.5\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    model.collect_params(), 'sgd',\n",
    "    {'learning_rate': args_lr, 'momentum': 0, 'wd': 0}\n",
    ")\n",
    "\n",
    "smce_loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Standard RNN Train and Evaluate Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_standard():\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = smce_loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = evaluate(val_data, model)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(test_data, model)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)\n",
    "    return\n",
    "\n",
    "\n",
    "def evaluate(data_source, model):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = smce_loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Standard LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 6.87, perplexity 962.37\n",
      "[Epoch 1 Batch 1000] loss 6.61, perplexity 738.91\n",
      "[Epoch 1 Batch 1500] loss 6.40, perplexity 602.46\n",
      "[Epoch 1 Batch 2000] loss 6.29, perplexity 538.11\n",
      "[Epoch 1 Batch 2500] loss 6.18, perplexity 480.82\n",
      "[Epoch 1 Batch 3000] loss 6.06, perplexity 428.86\n",
      "[Epoch 1 Batch 3500] loss 6.06, perplexity 426.40\n",
      "[Epoch 1 Batch 4000] loss 5.93, perplexity 375.06\n",
      "[Epoch 1 Batch 4500] loss 5.91, perplexity 368.07\n",
      "[Epoch 1 Batch 5000] loss 5.90, perplexity 364.07\n",
      "[Epoch 1 Batch 5500] loss 5.89, perplexity 362.32\n",
      "[Epoch 1] time cost 440.98s, validation loss 5.68, validation perplexity 292.24\n",
      "test loss 5.65, test perplexity 283.49\n",
      "Best test loss 5.65, test perplexity 283.49\n"
     ]
    }
   ],
   "source": [
    "train_standard()\n",
    "model.load_params(args_save, context)\n",
    "test_L = evaluate(test_data, model)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bayes-by-Backprop Classes\n",
    "\n",
    "Now we begin the Bayes-by-backprop portion of the chapter in earnest. To start, we define a Gaussian prior over the RNN model's parameters. It is initialized with a prior mean and standard deviation $\\sigma$, and has one important method, ``log_prob``, which computes the log-probability of the RNN model's paramters under the given prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_prob(x, mu, sigma):\n",
    "    return -0.5 * np.log(2.0 * np.pi) - mx.nd.log(sigma) - (x - mu) ** 2 / (2 * sigma ** 2)\n",
    "\n",
    "\n",
    "class Prior(object):\n",
    "\n",
    "    def __init__(self, prior_mu, prior_sigma):\n",
    "        self.prior_mu = mx.nd.array([prior_mu], ctx=context)\n",
    "        self.prior_sigma = mx.nd.array([prior_sigma], ctx=context)\n",
    "        return\n",
    "\n",
    "    def log_prob(self, model_params):\n",
    "        log_probs = [\n",
    "            mx.nd.sum(log_gaussian_prob(model_param, self.prior_mu, self.prior_sigma))\n",
    "            for model_param in model_params\n",
    "        ]\n",
    "        total_log_prob = log_probs[0]\n",
    "        for log_prob in log_probs[1:]:\n",
    "            total_log_prob = total_log_prob + log_prob\n",
    "        return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variational Posterior\n",
    "\n",
    "Next we define the variational posterior class. Like the prior class, the variational posterior is able to compute the log-probability of the RNN's parameters under its variational posterior distribution.\n",
    "\n",
    "However, unlike the prior class, the variational posterior's parameters get updated during training, whereas the prior's parameters are never updated. They are, after all, \"prior\".\n",
    "\n",
    "Additionally, the variational posterior provides a method to sample the RNN's parameters from its current posterior distribution. This is done with the ``sample_model_params`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarPosterior(object):\n",
    "\n",
    "    def __init__(self, model, var_sigma, var_mu_init_scale):\n",
    "        self.var_sigma = mx.nd.array([var_sigma], ctx=context)\n",
    "        self.var_mus = []\n",
    "        self.raw_var_mus = []\n",
    "        for i, model_param in enumerate(model.collect_params().values()):\n",
    "            mu = gluon.Parameter(\n",
    "                'mu_{}'.format(i), shape=model_param.shape,\n",
    "                init=mx.init.Normal(var_mu_init_scale))\n",
    "            mu.initialize(ctx=context)\n",
    "            self.var_mus.append(mu)\n",
    "            self.raw_var_mus.append(mu.data(context))\n",
    "        return\n",
    "\n",
    "    def sample_model_params(self):\n",
    "        model_params = []\n",
    "        for raw_var_mu in self.raw_var_mus:\n",
    "            epsilon = mx.nd.random_normal(shape=raw_var_mu.shape, loc=0., scale=1.0, ctx=context)\n",
    "            model_param = raw_var_mu + self.var_sigma * epsilon\n",
    "            model_params.append(model_param)\n",
    "        return model_params\n",
    "\n",
    "    def log_prob(self, model_params):\n",
    "        log_probs = [\n",
    "            mx.nd.sum(log_gaussian_prob(model_param, raw_var_mu, self.var_sigma))\n",
    "            for (model_param, raw_var_mu) in zip(model_params, self.raw_var_mus)\n",
    "        ]\n",
    "        total_log_prob = log_probs[0]\n",
    "        for log_prob in log_probs[1:]:\n",
    "            total_log_prob = total_log_prob + log_prob\n",
    "        return total_log_prob\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum([\n",
    "            np.prod(param.shape)\n",
    "            for param in self.var_mus\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Bayes-by-Backprop Loss\n",
    "\n",
    "As discussed, the Bayes-by-backprop loss is the sum of the expected log-likelihood on the training data and the KL-divergence between the variational posterior and the prior:\n",
    "\n",
    " ``INSERT FORMULA HERE``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBB_RNN_Loss(gluon.loss.Loss):\n",
    "\n",
    "    def __init__(self, prior, var_posterior, log_likelihood, num_batches, weight=None, batch_axis=0, **kwargs):\n",
    "        super(BBB_RNN_Loss, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self.prior = prior\n",
    "        self.var_posterior = var_posterior\n",
    "        self.log_likelihood = log_likelihood\n",
    "        self.num_batches = num_batches\n",
    "        return\n",
    "    \n",
    "    def forward(self, yhat, y, sampled_params, sample_weight=None):\n",
    "        neg_log_likelihood = mx.nd.sum(self.log_likelihood(yhat, y))\n",
    "        prior_log_prob = mx.nd.sum(self.prior.log_prob(sampled_params))\n",
    "        var_post_log_prob = mx.nd.sum(self.var_posterior.log_prob(sampled_params))\n",
    "        kl_loss = var_post_log_prob - prior_log_prob\n",
    "        var_loss = neg_log_likelihood + kl_loss / self.num_batches\n",
    "        return var_loss, neg_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize BBB-relevant classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dropout = 0.0\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "prior = Prior(0.0, 2)\n",
    "var_posterior = VarPosterior(model, 0.1, 0.1)\n",
    "bbb_rnn_loss = BBB_RNN_Loss(prior,\n",
    "                            var_posterior,\n",
    "                            gluon.loss.SoftmaxCrossEntropyLoss(),\n",
    "                            num_batches)\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    var_posterior.var_mus, 'sgd',\n",
    "    { 'learning_rate': args_lr, 'momentum': 0, 'wd': 0 }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the BBB-RNN training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bbb_rnn():\n",
    "    best_val = float(\"Inf\")\n",
    "\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            x, y = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "\n",
    "            with autograd.record():\n",
    "                sampled_params = var_posterior.sample_model_params()\n",
    "                model.set_params_to(sampled_params)\n",
    "                yhat, hidden = model(x, hidden)\n",
    "                var_loss, L = bbb_rnn_loss(yhat, y, sampled_params)\n",
    "                var_loss.backward()\n",
    "\n",
    "            grads = [var_mu.grad(context) for var_mu in var_posterior.var_mus]\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * var_posterior.num_params())\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        model.set_params_to(var_posterior.raw_var_mus)\n",
    "        val_L = evaluate(val_data, model)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            model.set_params_to(var_posterior.raw_var_mus)\n",
    "            test_L = evaluate(test_data, model)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start BBB training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 6.85, perplexity 942.83\n",
      "[Epoch 1 Batch 1000] loss 6.28, perplexity 533.58\n",
      "[Epoch 1 Batch 1500] loss 6.08, perplexity 436.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7fd0c380f4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_bbb_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_var_mus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_var_mus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best test loss %.2f, test perplexity %.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-33046251726c>\u001b[0m in \u001b[0;36mtrain_bbb_rnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar_mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_mus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_clip\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtotal_L\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/mxnet/gluon/utils.py\u001b[0m in \u001b[0;36mclip_global_norm\u001b[0;34m(arrays, max_norm)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/mxnet/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/mxnet/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_bbb_rnn()\n",
    "model.load_params(args_save, context)\n",
    "model.set_params_to(var_posterior.raw_var_mus)\n",
    "test_L = evaluate(test_data, model, var_posterior.raw_var_mus)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
